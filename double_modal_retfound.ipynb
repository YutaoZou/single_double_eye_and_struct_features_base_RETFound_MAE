{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f837183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取Excel数据，共 43445 条记录\n",
      "人口统计特征预处理后维度: 13\n",
      "训练集样本数: 0\n",
      "验证集样本数: 0\n",
      "测试集样本数: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 563\u001b[39m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m测试集准确率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 471\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# 创建数据加载器\u001b[39;00m\n\u001b[32m    470\u001b[39m batch_size = \u001b[32m16\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m    473\u001b[39m test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\retfound\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    352\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\retfound\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 1. 数据预处理 - 缺失值填充\n",
    "def fill_missing_values(df, is_train=True, fill_params=None):\n",
    "    \"\"\"填充缺失值，训练集计算填充参数，验证/测试集使用训练集参数\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # 数值型列填充（使用中位数抗异常值）\n",
    "    numeric_cols = ['townsend', 'bmi', 'baselineage']\n",
    "    if is_train:\n",
    "        fill_params = {}\n",
    "        for col in numeric_cols:\n",
    "            if col in df_filled.columns:\n",
    "                median_val = df_filled[col].median()\n",
    "                fill_params[f'{col}_median'] = median_val\n",
    "                df_filled[col].fillna(median_val, inplace=True)\n",
    "    else:\n",
    "        for col in numeric_cols:\n",
    "            if col in df_filled.columns and f'{col}_median' in fill_params:\n",
    "                df_filled[col].fillna(fill_params[f'{col}_median'], inplace=True)\n",
    "    \n",
    "    # 类别型列填充（使用众数或'unknown'）\n",
    "    categorical_cols = ['gender', 'smokingc', 'drinkc', 'ethnic', 'edu', \n",
    "                       'hbp', 'dmstatus', 'hyperlipidemia', \n",
    "                       'baseline_depression', 'incident_depression']\n",
    "    if is_train:\n",
    "        for col in categorical_cols:\n",
    "            if col not in df_filled.columns:\n",
    "                continue\n",
    "            missing_ratio = (df_filled[col].isnull().sum() / len(df_filled)) * 100\n",
    "            if missing_ratio < 50:\n",
    "                # 使用众数填充\n",
    "                mode_val = df_filled[col].mode()[0]\n",
    "                fill_params[f'{col}_mode'] = mode_val\n",
    "                df_filled[col].fillna(mode_val, inplace=True)\n",
    "            else:\n",
    "                # 缺失比例过高，使用'unknown'填充\n",
    "                fill_params[f'{col}_fill'] = 'unknown'\n",
    "                df_filled[col].fillna('unknown', inplace=True)\n",
    "    else:\n",
    "        for col in categorical_cols:\n",
    "            if col not in df_filled.columns:\n",
    "                continue\n",
    "            if f'{col}_mode' in fill_params:\n",
    "                df_filled[col].fillna(fill_params[f'{col}_mode'], inplace=True)\n",
    "            elif f'{col}_fill' in fill_params:\n",
    "                df_filled[col].fillna(fill_params[f'{col}_fill'], inplace=True)\n",
    "    \n",
    "    if is_train:\n",
    "        return df_filled, fill_params\n",
    "    else:\n",
    "        return df_filled\n",
    "\n",
    "# 2. 定义三分类标签函数\n",
    "def get_depression_class(row):\n",
    "    \"\"\"\n",
    "    三分类判断：\n",
    "    - 0: 无抑郁\n",
    "    - 1: 基线抑郁（baseline_depression为\"是\"或new_totdepress为1）\n",
    "    - 2: 新发抑郁（基线无抑郁，但incident_depression为\"是\"）\n",
    "    \"\"\"\n",
    "    # 基线抑郁（类别1）\n",
    "    if row['baseline_depression'] == '是' or row['new_totdepress'] == 1:\n",
    "        return 1\n",
    "    # 新发抑郁（类别2）\n",
    "    elif row['baseline_depression'] == '否' and row['new_totdepress'] == 0 and row['incident_depression'] == '是':\n",
    "        return 2\n",
    "    # 无抑郁（类别0）\n",
    "    elif row['baseline_depression'] == '否' and row['new_totdepress'] == 0 and row['incident_depression'] == '否':\n",
    "        return 0\n",
    "    # 异常情况\n",
    "    else:\n",
    "        print(f\"警告：数据异常 - 行索引{row.name}，基线抑郁:{row['baseline_depression']}, \"\n",
    "              f\"new_totdepress:{row['new_totdepress']}, 新发抑郁:{row['incident_depression']}，暂归为无抑郁\")\n",
    "        return 0\n",
    "\n",
    "# 3. 定义双模态数据集\n",
    "class DualModalDataset(Dataset):\n",
    "    def __init__(self, image_dirs, excel_df, preprocessor, transform=None, target_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        image_dirs: 包含三个类别的图像目录字典\n",
    "        excel_df: 包含人口统计特征和标签的DataFrame\n",
    "        preprocessor: 预处理管道（用于人口统计特征）\n",
    "        transform: 图像变换\n",
    "        target_size: 图像目标尺寸\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        # 构建图像路径和标签列表\n",
    "        for class_id, dir_path in image_dirs.items():\n",
    "            if not os.path.exists(dir_path):\n",
    "                continue\n",
    "            for img_file in os.listdir(dir_path):\n",
    "                if img_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # 提取eid_ckd从文件名\n",
    "                    eid_str = img_file.split('_')[1] if img_file.startswith(('left_', 'right_')) else img_file.split('_')[0]\n",
    "                    try:\n",
    "                        eid = int(eid_str)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    \n",
    "                    # 查找对应的人口统计数据\n",
    "                    if eid in excel_df['eid_ckd'].values:\n",
    "                        self.image_paths.append(os.path.join(dir_path, img_file))\n",
    "                        self.labels.append(class_id)\n",
    "        \n",
    "        # 创建eid到人口统计数据的映射\n",
    "        self.eid_demo_map = {row['eid_ckd']: row for _, row in excel_df.iterrows()}\n",
    "        \n",
    "        # 提取人口统计特征列\n",
    "        self.demo_cols = ['townsend', 'smokingc', 'drinkc', 'bmi', 'baselineage', \n",
    "                         'ethnic', 'edu', 'hbp', 'dmstatus', 'hyperlipidemia', 'gender']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 加载和预处理图像\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            with Image.open(img_path).convert('RGB') as img:\n",
    "                # 调整大小并中心裁剪\n",
    "                width, height = img.size\n",
    "                ratio = min(self.target_size[0]/width, self.target_size[1]/height)\n",
    "                new_size = (int(width * ratio), int(height * ratio))\n",
    "                img = img.resize(new_size, Image.LANCZOS)\n",
    "                \n",
    "                left = (new_size[0] - self.target_size[0]) // 2\n",
    "                top = (new_size[1] - self.target_size[1]) // 2\n",
    "                img = img.crop((left, top, left + self.target_size[0], top + self.target_size[1]))\n",
    "                \n",
    "                # 应用变换\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                \n",
    "                # 标准化\n",
    "                img = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                         std=[0.229, 0.224, 0.225])(img)\n",
    "        except Exception as e:\n",
    "            print(f\"处理图像 {img_path} 出错: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # 提取和预处理人口统计特征\n",
    "        img_file = os.path.basename(img_path)\n",
    "        eid_str = img_file.split('_')[1] if img_file.startswith(('left_', 'right_')) else img_file.split('_')[0]\n",
    "        try:\n",
    "            eid = int(eid_str)\n",
    "        except ValueError:\n",
    "            return None\n",
    "            \n",
    "        if eid not in self.eid_demo_map:\n",
    "            return None\n",
    "            \n",
    "        demo_data = self.eid_demo_map[eid]\n",
    "        demo_df = pd.DataFrame([demo_data[self.demo_cols].values], columns=self.demo_cols)\n",
    "        demo_features = self.preprocessor.transform(demo_df)[0]\n",
    "        demo_features = torch.tensor(demo_features, dtype=torch.float32)\n",
    "        \n",
    "        # 获取标签\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return img, demo_features, label\n",
    "\n",
    "# 4. 定义双模态RetFound模型\n",
    "class DualModalRetFound(nn.Module):\n",
    "    def __init__(self, demo_feature_dim, num_classes=3, freeze_retfound=True):\n",
    "        super(DualModalRetFound, self).__init__()\n",
    "        # 加载预训练的RetFound模型\n",
    "        self.retfound = timm.create_model(\n",
    "            'vit_large_patch16_224_retfound',\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # 不加载分类头\n",
    "            in_chans=3\n",
    "        )\n",
    "        self.image_feature_dim = self.retfound.num_features\n",
    "        \n",
    "        # 冻结RetFound的参数（微调初期）\n",
    "        if freeze_retfound:\n",
    "            for param in self.retfound.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 人口统计特征处理分支\n",
    "        self.demo_branch = nn.Sequential(\n",
    "            nn.Linear(demo_feature_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, self.image_feature_dim)\n",
    "        )\n",
    "        \n",
    "        # 特征融合层\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim * 2, self.image_feature_dim),\n",
    "            nn.BatchNorm1d(self.image_feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Linear(self.image_feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, images, demo_features):\n",
    "        # 提取图像特征\n",
    "        img_feat = self.retfound(images)\n",
    "        \n",
    "        # 处理人口统计特征\n",
    "        demo_feat = self.demo_branch(demo_features)\n",
    "        \n",
    "        # 融合特征\n",
    "        fused_feat = torch.cat([img_feat, demo_feat], dim=1)\n",
    "        fused_feat = self.fusion(fused_feat)\n",
    "        \n",
    "        # 分类\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# 5. 训练和评估函数\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device='cuda'):\n",
    "    \"\"\"训练模型并在验证集上评估\"\"\"\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # 记录训练过程\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            images, demo_feats, labels = batch\n",
    "            images = images.to(device)\n",
    "            demo_feats = demo_feats.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(images, demo_feats)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # 反向传播和优化\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # 统计\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            images, demo_feats, labels = batch\n",
    "            images = images.to(device)\n",
    "            demo_feats = demo_feats.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(images, demo_feats)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 统计\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            running_val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # 收集所有预测和标签用于详细评估\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_acc = running_val_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        \n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = model.state_dict()\n",
    "        \n",
    "        # 打印详细分类报告\n",
    "        print(\"\\n验证集分类报告:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=['无抑郁', '基线抑郁', '新发抑郁']))\n",
    "        \n",
    "        # 绘制混淆矩阵\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['无抑郁', '基线抑郁', '新发抑郁'],\n",
    "                   yticklabels=['无抑郁', '基线抑郁', '新发抑郁'])\n",
    "        plt.xlabel('预测标签')\n",
    "        plt.ylabel('真实标签')\n",
    "        plt.title(f'第{epoch+1}轮混淆矩阵')\n",
    "        plt.show()\n",
    "    \n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, history\n",
    "\n",
    "# 6. 主函数\n",
    "def main():\n",
    "    # 配置路径\n",
    "    EXCEL_PATH = \"D:/LingYi/0820_ukb_depression_fundus.xlsx\"\n",
    "    IMAGE_BASE_DIR = \"D:/LingYi/data_pre_depression_by_right_eye\"\n",
    "    SAVE_DIR = \"D:/LingYi/retfound_results\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # 读取Excel数据\n",
    "    df = pd.read_excel(EXCEL_PATH)\n",
    "    print(f\"成功读取Excel数据，共 {len(df)} 条记录\")\n",
    "    \n",
    "    # 按eid_ckd拆分训练集、验证集和测试集\n",
    "    eids = df['eid_ckd'].unique()\n",
    "    train_eids, temp_eids = train_test_split(eids, test_size=0.3, random_state=SEED)\n",
    "    val_eids, test_eids = train_test_split(temp_eids, test_size=0.5, random_state=SEED)\n",
    "    \n",
    "    # 拆分数据集\n",
    "    train_df = df[df['eid_ckd'].isin(train_eids)]\n",
    "    val_df = df[df['eid_ckd'].isin(val_eids)]\n",
    "    test_df = df[df['eid_ckd'].isin(test_eids)]\n",
    "    \n",
    "    # 填充缺失值\n",
    "    train_df_filled, fill_params = fill_missing_values(train_df, is_train=True)\n",
    "    val_df_filled = fill_missing_values(val_df, is_train=False, fill_params=fill_params)\n",
    "    test_df_filled = fill_missing_values(test_df, is_train=False, fill_params=fill_params)\n",
    "    \n",
    "    # 保存填充参数\n",
    "    # 将NumPy类型转换为Python原生类型\n",
    "    def convert_numpy_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # 转换所有参数类型\n",
    "    fill_params_converted = {k: convert_numpy_types(v) for k, v in fill_params.items()}\n",
    "\n",
    "    # 保存转换后的参数\n",
    "    with open(os.path.join(SAVE_DIR, 'fill_params.json'), 'w') as f:\n",
    "        json.dump(fill_params_converted, f, indent=4)\n",
    "    \n",
    "    # 为每个样本生成三分类标签\n",
    "    train_df_filled['label'] = train_df_filled.apply(get_depression_class, axis=1)\n",
    "    val_df_filled['label'] = val_df_filled.apply(get_depression_class, axis=1)\n",
    "    test_df_filled['label'] = test_df_filled.apply(get_depression_class, axis=1)\n",
    "    \n",
    "    # 定义人口统计特征预处理管道\n",
    "    numeric_features = ['townsend', 'bmi', 'baselineage']\n",
    "    categorical_features = ['gender', 'smokingc', 'drinkc', 'ethnic', 'edu', \n",
    "                           'hbp', 'dmstatus', 'hyperlipidemia']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False, drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # 拟合预处理管道（仅使用训练集）\n",
    "    preprocessor.fit(train_df_filled[numeric_features + categorical_features])\n",
    "    demo_feature_dim = preprocessor.transform(train_df_filled[numeric_features + categorical_features]).shape[1]\n",
    "    print(f\"人口统计特征预处理后维度: {demo_feature_dim}\")\n",
    "    \n",
    "    # 定义图像变换\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=(-5, 5)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # 定义训练/验证/测试集的图像目录\n",
    "    def get_image_dirs(base_dir, dataset_type):\n",
    "        return {\n",
    "            0: os.path.join(base_dir, dataset_type, 'class_0_non_depression'),\n",
    "            1: os.path.join(base_dir, dataset_type, 'class_1_baseline_dep'),\n",
    "            2: os.path.join(base_dir, dataset_type, 'class_2_incident_dep')\n",
    "        }\n",
    "    \n",
    "    train_image_dirs = get_image_dirs(IMAGE_BASE_DIR, 'train')\n",
    "    val_image_dirs = get_image_dirs(IMAGE_BASE_DIR, 'val')\n",
    "    test_image_dirs = get_image_dirs(IMAGE_BASE_DIR, 'test')\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = DualModalDataset(\n",
    "        train_image_dirs, train_df_filled, preprocessor, \n",
    "        transform=train_transform, target_size=(224, 224)\n",
    "    )\n",
    "    val_dataset = DualModalDataset(\n",
    "        val_image_dirs, val_df_filled, preprocessor, \n",
    "        transform=val_test_transform, target_size=(224, 224)\n",
    "    )\n",
    "    test_dataset = DualModalDataset(\n",
    "        test_image_dirs, test_df_filled, preprocessor, \n",
    "        transform=val_test_transform, target_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(f\"训练集样本数: {len(train_dataset)}\")\n",
    "    print(f\"验证集样本数: {len(val_dataset)}\")\n",
    "    print(f\"测试集样本数: {len(test_dataset)}\")\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 初始化模型\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    model = DualModalRetFound(\n",
    "        demo_feature_dim=demo_feature_dim,\n",
    "        num_classes=3,\n",
    "        freeze_retfound=True  # 初始冻结RetFound\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"\\n开始训练模型...\")\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        criterion, optimizer, \n",
    "        num_epochs=20,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'dual_modal_retfound.pth'))\n",
    "    \n",
    "    # 绘制训练历史\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='训练损失')\n",
    "    plt.plot(history['val_loss'], label='验证损失')\n",
    "    plt.title('损失曲线')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='训练准确率')\n",
    "    plt.plot(history['val_acc'], label='验证准确率')\n",
    "    plt.title('准确率曲线')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'training_history.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    print(\"\\n在测试集上评估模型...\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            images, demo_feats, labels = batch\n",
    "            images = images.to(device)\n",
    "            demo_feats = demo_feats.to(device)\n",
    "            \n",
    "            outputs = model(images, demo_feats)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # 打印测试集结果\n",
    "    print(\"\\n测试集分类报告:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['无抑郁', '基线抑郁', '新发抑郁']))\n",
    "    \n",
    "    # 绘制测试集混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['无抑郁', '基线抑郁', '新发抑郁'],\n",
    "               yticklabels=['无抑郁', '基线抑郁', '新发抑郁'])\n",
    "    plt.xlabel('预测标签')\n",
    "    plt.ylabel('真实标签')\n",
    "    plt.title('测试集混淆矩阵')\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'test_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"测试集准确率: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac2ea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据形状: (43445, 18)\n",
      "使用的人口统计学特征: ['townsend', 'smokingc', 'drinkc', 'bmi', 'baselineage', 'ethnic', 'edu', 'hbp', 'dmstatus', 'hyperlipidemia', 'gender']\n",
      "\n",
      "标签分布:\n",
      "label\n",
      "0    39503\n",
      "1     2540\n",
      "2     1402\n",
      "Name: count, dtype: int64\n",
      "0: 无抑郁, 1: 基线抑郁, 2: 新发抑郁\n",
      "\n",
      "数据集分割:\n",
      "训练集: (28239, 11), 验证集: (6516, 11), 测试集: (8690, 11)\n",
      "\n",
      "开始模型训练和超参数搜索...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳参数: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.05, 'classifier__max_depth': 8, 'classifier__min_child_samples': 5, 'classifier__n_estimators': 400, 'classifier__num_leaves': 63, 'classifier__subsample': 0.8}\n",
      "训练集最佳F1分数: 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "验证集评估:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         无抑郁       0.92      0.46      0.61      5925\n",
      "        基线抑郁       0.08      0.36      0.12       381\n",
      "        新发抑郁       0.03      0.27      0.06       210\n",
      "\n",
      "    accuracy                           0.45      6516\n",
      "   macro avg       0.34      0.36      0.27      6516\n",
      "weighted avg       0.84      0.45      0.57      6516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试集评估:\n",
      "准确率: 0.4574\n",
      "宏平均F1分数: 0.2756\n",
      "宏平均精确率: 0.3490\n",
      "宏平均召回率: 0.4000\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         无抑郁       0.93      0.47      0.62      7901\n",
      "        基线抑郁       0.08      0.35      0.12       508\n",
      "        新发抑郁       0.05      0.38      0.08       281\n",
      "\n",
      "    accuracy                           0.46      8690\n",
      "   macro avg       0.35      0.40      0.28      8690\n",
      "weighted avg       0.85      0.46      0.57      8690\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 243>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5折交叉验证F1分数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    230\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train_random_forest(X_train, y_train, X_val, y_val, preprocessor)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m    236\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_model, MODEL_SAVE_PATH)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_test, y_test, feature_names)\u001b[0m\n\u001b[0;32m    180\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n\u001b[0;32m    181\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m--> 182\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mheatmap(\n\u001b[0;32m    183\u001b[0m     cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    184\u001b[0m     xticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m无抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m基线抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m新发抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m     yticklabels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m无抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m基线抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m新发抑郁\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    186\u001b[0m )\n\u001b[0;32m    187\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m预测标签\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    188\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m真实标签\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             f1_score, recall_score, precision_score)\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.combine import SMOTETomek  # 结合过采样和欠采样\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 正确显示负号\n",
    "\n",
    "def load_and_preprocess_data(excel_path):\n",
    "    \"\"\"加载数据并进行预处理\"\"\"\n",
    "    # 读取Excel数据\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(f\"原始数据形状: {df.shape}\")\n",
    "    \n",
    "    # 定义人口统计学特征列（根据实际数据调整）\n",
    "    demographic_cols = [\n",
    "        'townsend', 'smokingc', 'drinkc', 'bmi', \n",
    "        'baselineage', 'ethnic', 'edu', 'hbp', 'dmstatus', 'hyperlipidemia', 'gender'\n",
    "    ]\n",
    "    \n",
    "    # 检查并保留存在的列\n",
    "    existing_cols = [col for col in demographic_cols if col in df.columns]\n",
    "    print(f\"使用的人口统计学特征: {existing_cols}\")\n",
    "    \n",
    "    # 三分类标签函数\n",
    "    def classify_depression(row):\n",
    "        \"\"\"\n",
    "        三分类判断：\n",
    "        - 0: 无抑郁\n",
    "        - 1: 基线抑郁\n",
    "        - 2: 新发抑郁\n",
    "        \"\"\"\n",
    "#         baseline_depressed = (pd.notna(row['new_totdepress']) and row['new_totdepress'] == 1) or \\\n",
    "        baseline_depressed =                    (pd.notna(row['baseline_depression']) and row['baseline_depression'] == '是')\n",
    "        \n",
    "        incident_depressed = pd.notna(row['incident_depression']) and row['incident_depression'] == '是'\n",
    "        \n",
    "        if baseline_depressed:\n",
    "            return 1  # 基线抑郁\n",
    "        elif not baseline_depressed and incident_depressed:\n",
    "            return 2  # 新发抑郁\n",
    "        else:\n",
    "            return 0  # 无抑郁\n",
    "    \n",
    "    # 创建标签列\n",
    "    df['label'] = df.apply(classify_depression, axis=1)\n",
    "    \n",
    "    # 检查标签分布\n",
    "    print(\"\\n标签分布:\")\n",
    "    print(df['label'].value_counts().sort_index())\n",
    "    print(f\"0: 无抑郁, 1: 基线抑郁, 2: 新发抑郁\")\n",
    "    \n",
    "    # 分离特征和标签\n",
    "    X = df[existing_cols].copy()\n",
    "    y = df['label'].copy()\n",
    "    \n",
    "    # 填充缺失值（修正警告版本）\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "    # 数值型用中位数填充（不使用inplace=True）\n",
    "    for col in numeric_cols:\n",
    "        X[col] = X[col].fillna(X[col].median())  # 直接赋值替换\n",
    "\n",
    "    # 类别型用众数填充（不使用inplace=True）\n",
    "    for col in categorical_cols:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])  # 直接赋值替换\n",
    "    \n",
    "    return X, y, numeric_cols, categorical_cols\n",
    "\n",
    "def split_dataset(X, y, test_size=0.2, val_size=0.15, random_state=42):\n",
    "    \"\"\"分割训练集、验证集和测试集\"\"\"\n",
    "    # 先分割训练集和临时集\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=test_size + val_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 从临时集中分割验证集和测试集\n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=1 - val_ratio, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n数据集分割:\")\n",
    "    print(f\"训练集: {X_train.shape}, 验证集: {X_val.shape}, 测试集: {X_test.shape}\")\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def build_preprocessing_pipeline(numeric_cols, categorical_cols):\n",
    "    \"\"\"构建预处理管道\"\"\"\n",
    "    # 定义预处理步骤\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),  # 数值特征标准化\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)  # 类别特征独热编码\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, preprocessor):\n",
    "    \"\"\"训练随机森林模型并进行超参数调优\"\"\"\n",
    "    # 构建包含预处理和LightGBM模型的管道\n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('sampler', SMOTETomek(random_state=42)),  # 对训练集进行重采样\n",
    "        ('classifier', lgb.LGBMClassifier(\n",
    "            # 自定义类别权重：无抑郁=1，基线抑郁=15，新发抑郁=25（根据样本比例设置）\n",
    "            class_weight={0:1, 1:15, 2:25},\n",
    "            objective='multiclass',\n",
    "            num_class=3,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        ))\n",
    "        \n",
    "    ])\n",
    "\n",
    "    # 修正后的LightGBM超参数搜索空间\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [400],  # 增加树数量，避免欠拟合\n",
    "        'classifier__learning_rate': [0.05],\n",
    "        'classifier__max_depth': [8],\n",
    "        'classifier__num_leaves': [63],  # 控制树复杂度\n",
    "        'classifier__min_child_samples': [5],  # 防止过拟合\n",
    "        'classifier__subsample': [0.8],\n",
    "        'classifier__colsample_bytree': [1.0]  # 特征采样，增加多样性\n",
    "    }\n",
    "    \n",
    "    # 网格搜索\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n开始模型训练和超参数搜索...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳参数: {grid_search.best_params_}\")\n",
    "    print(f\"训练集最佳F1分数: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 在验证集上评估\n",
    "    val_preds = grid_search.predict(X_val)\n",
    "    print(\"\\n验证集评估:\")\n",
    "    print(classification_report(y_val, val_preds, target_names=['无抑郁', '基线抑郁', '新发抑郁']))\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, feature_names):\n",
    "    \"\"\"评估模型并可视化结果\"\"\"\n",
    "    # 在测试集上预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    print(\"\\n测试集评估:\")\n",
    "    print(f\"准确率: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"宏平均F1分数: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"宏平均精确率: {precision_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"宏平均召回率: {recall_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    \n",
    "    # 详细分类报告\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(\n",
    "        y_test, y_pred, \n",
    "        target_names=['无抑郁', '基线抑郁', '新发抑郁']\n",
    "    ))\n",
    "    \n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues', \n",
    "        xticklabels=['无抑郁', '基线抑郁', '新发抑郁'],\n",
    "        yticklabels=['无抑郁', '基线抑郁', '新发抑郁']\n",
    "    )\n",
    "    plt.xlabel('预测标签')\n",
    "    plt.ylabel('真实标签')\n",
    "    plt.title('测试集混淆矩阵')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 特征重要性（如果是树模型）\n",
    "    if hasattr(model['classifier'], 'feature_importances_'):\n",
    "        # 获取预处理后的特征名\n",
    "        ohe = model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "        cat_features = list(ohe.get_feature_names_out(categorical_cols))\n",
    "        all_features = numeric_cols + cat_features\n",
    "        \n",
    "        # 提取特征重要性\n",
    "        importances = model['classifier'].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # 可视化特征重要性\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(len(indices[:10])), importances[indices[:10]], color='b', align='center')\n",
    "        plt.yticks(range(len(indices[:10])), [all_features[i] for i in indices[:10]])\n",
    "        plt.xlabel('特征重要性')\n",
    "        plt.title('随机森林特征重要性（前10名）')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # 配置路径\n",
    "    EXCEL_PATH = \"D:/LingYi/0820_ukb_depression_fundus.xlsx\"  # 替换为你的Excel路径\n",
    "    MODEL_SAVE_PATH = \"demographic_rf_model.pkl\"\n",
    "    \n",
    "    # 加载和预处理数据\n",
    "    X, y, numeric_cols, categorical_cols = load_and_preprocess_data(EXCEL_PATH)\n",
    "    \n",
    "    # 分割数据集\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y)\n",
    "    \n",
    "    # 构建预处理管道\n",
    "    preprocessor = build_preprocessing_pipeline(numeric_cols, categorical_cols)\n",
    "    \n",
    "    # 训练模型\n",
    "    best_model = train_random_forest(X_train, y_train, X_val, y_val, preprocessor)\n",
    "    \n",
    "    # 评估模型\n",
    "    evaluate_model(best_model, X_test, y_test, X.columns)\n",
    "    \n",
    "    # 保存模型\n",
    "    joblib.dump(best_model, MODEL_SAVE_PATH)\n",
    "    print(f\"\\n模型已保存至: {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # 交叉验证\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1_macro')\n",
    "    print(f\"\\n5折交叉验证F1分数: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e50caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a856ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\retfound\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from transformers import AutoImageProcessor  # RETFound官方推荐的图像预处理工具\n",
    "\n",
    "class BilateralRetinaDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", num_classes=2, image_processor=None):\n",
    "        \"\"\"\n",
    "        适配你生成的双眼数据集目录结构：\n",
    "        root_dir/\n",
    "        ├─ train/\n",
    "        │  ├─ bilateral/\n",
    "        │  │  ├─ class_depression/\n",
    "        │  │  │  ├─ bilateral_1001_0_left.jpg\n",
    "        │  │  │  ├─ bilateral_1001_0_right.jpg\n",
    "        │  │  └─ class_non_depression/\n",
    "        ├─ val/...\n",
    "        \"\"\"\n",
    "        self.root = os.path.join(root_dir, split, \"bilateral\")  # 双眼配对目录\n",
    "        self.num_classes = num_classes\n",
    "        self.image_processor = image_processor  # RETFound的图像预处理（替代自定义transform）\n",
    "        self.class_map = {\"class_depression\": 1, \"class_non_depression\": 0}  # 标签映射\n",
    "        self.pair_list = self._collect_bilateral_pairs()  # 收集所有双眼图像对\n",
    "\n",
    "    def _collect_bilateral_pairs(self):\n",
    "        \"\"\"按文件名前缀匹配左眼和右眼图像对（核心：对齐你生成的文件名格式）\"\"\"\n",
    "        pair_list = []\n",
    "        # 遍历抑郁/非抑郁两类目录\n",
    "        for cls_name, cls_label in self.class_map.items():\n",
    "            cls_dir = os.path.join(self.root, cls_name)\n",
    "            if not os.path.exists(cls_dir):\n",
    "                continue\n",
    "            # 找到所有左眼图像，通过文件名替换匹配右眼图像\n",
    "            left_imgs = glob.glob(os.path.join(cls_dir, \"*_left.jpg\"))\n",
    "            for left_path in left_imgs:\n",
    "                # 替换_left.jpg为_right.jpg，匹配同组右眼图像\n",
    "                right_path = left_path.replace(\"_left.jpg\", \"_right.jpg\")\n",
    "                if os.path.exists(right_path):\n",
    "                    pair_list.append((left_path, right_path, cls_label))\n",
    "        print(f\"[{self.root}] 收集到 {len(pair_list)} 个双眼图像对\")\n",
    "        return pair_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left_path, right_path, label = self.pair_list[idx]\n",
    "        # 加载图像（RGB格式，与预处理的224×224一致）\n",
    "        left_img = Image.open(left_path).convert(\"RGB\")\n",
    "        right_img = Image.open(right_path).convert(\"RGB\")\n",
    "        \n",
    "        # 使用RETFound官方ImageProcessor预处理（避免与预训练权重冲突）\n",
    "        # 输出为模型所需的pixel_values（tensor格式，shape: (3, 224, 224)）\n",
    "        left_processed = self.image_processor(\n",
    "            left_img, return_tensors=\"pt\", do_resize=False, do_center_crop=False\n",
    "        )[\"pixel_values\"].squeeze(0)  # 你已提前裁剪为224×224，关闭resize/crop\n",
    "        right_processed = self.image_processor(\n",
    "            right_img, return_tensors=\"pt\", do_resize=False, do_center_crop=False\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # 返回字典格式，与模型输入（batch[\"left_img\"]等）完全匹配\n",
    "        return {\n",
    "            \"left_img\": left_processed,\n",
    "            \"right_img\": right_processed,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 初始化DataLoader（训练/验证/测试集通用）\n",
    "def build_bilateral_dataloader(args, split=\"train\"):\n",
    "    # 加载RETFound官方图像处理器（确保与预训练权重的预处理一致）\n",
    "    from transformers import AutoImageProcessor\n",
    "    image_processor = AutoImageProcessor.from_pretrained(args.finetune)\n",
    "    \n",
    "    # 初始化数据集\n",
    "    dataset = BilateralRetinaDataset(\n",
    "        root_dir=args.data_path,\n",
    "        split=split,\n",
    "        num_classes=args.nb_classes,\n",
    "        image_processor=image_processor\n",
    "    )\n",
    "    \n",
    "    # 构建DataLoader\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=(split == \"train\"),  # 训练集打乱，验证/测试集不打乱\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199484f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "def build_model(args):\n",
    "    model_type = args.model\n",
    "    pretrained_path = args.finetune  # RETFound预训练权重路径（如\"microsoft/retfound-mae-base\"）\n",
    "    \n",
    "    if model_type == \"RETFound_mae_bilateral\":\n",
    "        # 1. 加载RETFound MAE预训练配置和骨干网络\n",
    "        # 关键：指定output_hidden_states=True，确保能获取last_hidden_state\n",
    "        config = AutoConfig.from_pretrained(pretrained_path, output_hidden_states=True)\n",
    "        base_model = AutoModel.from_pretrained(pretrained_path, config=config)\n",
    "        feature_dim = config.hidden_size  # RETFound的CLS特征维度（如768/1024）\n",
    "        \n",
    "        # 2. 定义双眼特征融合模型（沿用你的核心逻辑，补充细节）\n",
    "        class BilateralRETFoundModel(nn.Module):\n",
    "            def __init__(self, base_model, feature_dim, num_classes, drop_path=0.1):\n",
    "                super().__init__()\n",
    "                self.base_model = base_model\n",
    "                self.num_classes = num_classes\n",
    "                \n",
    "                # 双眼特征融合模块（参考论文：自适应线性融合+BN+Dropout）\n",
    "                self.fusion = nn.Sequential(\n",
    "                    nn.Linear(feature_dim * 2, feature_dim),  # 2*feat_dim → feat_dim\n",
    "                    nn.BatchNorm1d(feature_dim),  # 批量归一化，稳定训练\n",
    "                    nn.ReLU(inplace=True),  # 激活函数\n",
    "                    nn.Dropout(drop_path)  # 防止过拟合，与args.drop_path对齐\n",
    "                )\n",
    "                \n",
    "                # 分类头（与RETFound微调任务一致，使用线性层）\n",
    "                self.class_head = nn.Linear(feature_dim, num_classes)\n",
    "                \n",
    "                # 3. 冻结预训练骨干网络底层（可选，小样本场景推荐）\n",
    "                self._freeze_backbone_layers(freeze_ratio=0.7)  # 冻结70%的Transformer层\n",
    "\n",
    "            def _freeze_backbone_layers(self, freeze_ratio=0.7):\n",
    "                \"\"\"冻结RETFound骨干网络的底层Transformer层（避免权重破坏）\"\"\"\n",
    "                num_layers = len(self.base_model.encoder.layer)  # Transformer层总数\n",
    "                freeze_layers = int(num_layers * freeze_ratio)\n",
    "                for i, layer in enumerate(self.base_model.encoder.layer):\n",
    "                    if i < freeze_layers:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = False\n",
    "                print(f\"冻结RETFound底层 {freeze_layers}/{num_layers} 个Transformer层\")\n",
    "\n",
    "            def extract_single_feat(self, img_tensor):\n",
    "                \"\"\"提取单眼图像的CLS特征（适配RETFound输出格式）\"\"\"\n",
    "                # img_tensor: (batch_size, 3, 224, 224)\n",
    "                outputs = self.base_model(pixel_values=img_tensor)\n",
    "                # 获取CLS Token的特征（last_hidden_state的第0个token）\n",
    "                cls_feat = outputs.last_hidden_state[:, 0, :]  # (batch_size, feature_dim)\n",
    "                return cls_feat\n",
    "\n",
    "            def forward(self, batch):\n",
    "                \"\"\"\n",
    "                输入：batch字典（left_img/right_img/label）\n",
    "                输出：logits（预测概率）、labels（真实标签）\n",
    "                \"\"\"\n",
    "                # 从batch中获取双眼图像和标签\n",
    "                left_imgs = batch[\"left_img\"]  # (batch_size, 3, 224, 224)\n",
    "                right_imgs = batch[\"right_img\"]  # (batch_size, 3, 224, 224)\n",
    "                labels = batch[\"label\"]  # (batch_size,)\n",
    "                \n",
    "                # 独立提取双眼特征\n",
    "                left_feat = self.extract_single_feat(left_imgs)  # (bs, feat_dim)\n",
    "                right_feat = self.extract_single_feat(right_imgs)  # (bs, feat_dim)\n",
    "                \n",
    "                # 特征融合：拼接→线性压缩→分类\n",
    "                fused_feat = torch.cat([left_feat, right_feat], dim=1)  # (bs, 2*feat_dim)\n",
    "                fused_feat = self.fusion(fused_feat)  # (bs, feat_dim)\n",
    "                logits = self.class_head(fused_feat)  # (bs, num_classes)\n",
    "                \n",
    "                return logits, labels\n",
    "        \n",
    "        # 4. 初始化双眼模型\n",
    "        model = BilateralRETFoundModel(\n",
    "            base_model=base_model,\n",
    "            feature_dim=feature_dim,\n",
    "            num_classes=args.nb_classes,\n",
    "            drop_path=args.drop_path  # 从args传入Dropout比例，确保配置统一\n",
    "        )\n",
    "        print(f\"成功初始化双眼RETFound模型，分类头维度：{feature_dim}→{args.nb_classes}\")\n",
    "    \n",
    "    else:\n",
    "        # 保留原单眼模型逻辑（兼容其他模型类型）\n",
    "        model = create_retfound_mae_model(args)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebd11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] --finetune FINETUNE [--nb_classes NB_CLASSES]\n",
      "                             [--drop_path DROP_PATH] --data_path DATA_PATH [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--epochs EPOCHS] [--lr LR] [--weight_decay WEIGHT_DECAY]\n",
      "                             [--log_interval LOG_INTERVAL] [--save_dir SAVE_DIR]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --finetune, --data_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\retfound\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(args, model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # 1. 将batch数据移到GPU（若使用GPU）\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # 2. 模型前向传播（使用你定义的forward逻辑）\n",
    "        logits, labels = model(batch)\n",
    "        \n",
    "        # 3. 计算损失（二分类任务用CrossEntropyLoss）\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # 4. 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. 统计训练指标\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # 打印训练日志（每10个batch打印一次）\n",
    "        if (batch_idx + 1) % args.log_interval == 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_acc = total_correct / total_samples\n",
    "            print(f\"Train Batch [{batch_idx+1}/{len(dataloader)}] | Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    # 返回epoch级指标\n",
    "    epoch_avg_loss = total_loss / total_samples\n",
    "    epoch_avg_acc = total_correct / total_samples\n",
    "    return epoch_avg_loss, epoch_avg_acc\n",
    "\n",
    "# 主训练函数（简化版）\n",
    "def main(args):\n",
    "    # 初始化设备（CPU/GPU）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. 构建双眼DataLoader\n",
    "    train_loader = build_bilateral_dataloader(args, split=\"train\")\n",
    "    val_loader = build_bilateral_dataloader(args, split=\"val\")\n",
    "    \n",
    "    # 2. 构建模型（使用你的双眼模型）\n",
    "    model = build_model(args).to(device)\n",
    "    \n",
    "    # 3. 初始化损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()  # 二分类任务\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay  # 正则化，防止过拟合\n",
    "    )\n",
    "    \n",
    "    # 4. 开始训练\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"\\n=== Epoch [{epoch+1}/{args.epochs}] ===\")\n",
    "        # 训练一个epoch\n",
    "        train_loss, train_acc = train_one_epoch(args, model, train_loader, criterion, optimizer, device)\n",
    "        # 验证一个epoch（逻辑类似train_one_epoch，仅需关闭梯度计算）\n",
    "        val_loss, val_acc = validate_one_epoch(args, model, val_loader, criterion, device)\n",
    "        \n",
    "        # 保存最佳模型（按验证集准确率）\n",
    "        if val_acc > args.best_val_acc:\n",
    "            args.best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(args.save_dir, \"best_bilateral_model.pth\"))\n",
    "            print(f\"保存最佳模型，验证准确率：{val_acc:.4f}\")\n",
    "\n",
    "# 命令行参数解析（补充必要参数）\n",
    "def parse_args():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 模型配置\n",
    "    parser.add_argument(\"--model\", type=str, default=\"RETFound_mae_bilateral\", help=\"模型类型\")\n",
    "    parser.add_argument(\"--finetune\", type=str, required=True, help=\"RETFound预训练权重路径\")\n",
    "    parser.add_argument(\"--nb_classes\", type=int, default=2, help=\"分类数（抑郁/非抑郁）\")\n",
    "    parser.add_argument(\"--drop_path\", type=float, default=0.1, help=\"Dropout比例\")\n",
    "    # 数据配置\n",
    "    parser.add_argument(\"--data_path\", type=str, required=True, help=\"双眼数据集根目录\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"批次大小\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"数据加载线程数\")\n",
    "    # 训练配置\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"训练轮数\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"学习率\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-5, help=\"权重衰减\")\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=10, help=\"日志打印间隔\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./save_models\", help=\"模型保存目录\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    # 创建模型保存目录\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    # 初始化最佳验证准确率\n",
    "    args.best_val_acc = 0.0\n",
    "    # 启动训练\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d4283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece77a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python311_retfound",
   "language": "python",
   "name": "retfound"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
